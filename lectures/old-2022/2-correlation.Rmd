---
title: 'Correlations'
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: [default, rladies, rladies-fonts, "my-theme.css"]
    incremental: true
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false

---
## Recap

```{r, echo = F}
library(psych)
options(scipen=999)
knitr::opts_chunk$set(message = F, warning = F)
```

Correlations are:

- Standardized covariances
     
     + Range from -1 to 1
     
- an effect size

    + Measure of the strength of association between two continuous variables
    
- Calculation:
  - Sum the cross-product of deviation scores
  - Divide by N-1 
  - Divide by the product of standard deviation scores
     
---

### Example

Do Pulizters help newspapers keep readers? (Data from [FiveThirtyEight](https://fivethirtyeight.com/features/do-pulitzers-help-newspapers-keep-readers/)). 

```{r}
library(fivethirtyeight)
data("pulitzer")
head(pulitzer)
```

---

```{r}
x_var = pulitzer$pctchg_circ
y_var = pulitzer$num_finals2004_2014 
n = length(x_var)

x_d = x_var - mean(x_var)
y_d = y_var - mean(y_var)

describe(cbind(x_var, x_d, y_var, y_d), fast = T)
```
---

```{r}
# cross products
x_d*y_d

# sum of cross products (variation)
sum(x_d*y_d)

# variance
sum(x_d*y_d)/( n-1 )

# correlation

( sum(x_d*y_d)/( n-1 ) ) / ( sd(x_var)*sd(y_var) )
```

---

```{r}
cor(pulitzer$pctchg_circ,
    pulitzer$num_finals2004_2014)

cor.test(pulitzer$pctchg_circ,
    pulitzer$num_finals2004_2014)
```

_Note: `cor.test` cannot handle a null hypothesis other than 0. You'll have to calculate significance by hand if you're interested in using another null._


---

### Recap: testing the significance of a correlation

.pull-left[
If the null hypothesis is the .purple[nil hypothesis]:
 
 - test significance using a _t_-distribution, where 
 
 $$\large t = \frac{r}{SE_r}$$
 $$\large SE_r = \sqrt{\frac{1-r^2}{N-2}}$$
 
 $$DF = N-2$$
 ]
 
 .pull-right[
 If null hypothesis is not 0 $(\text{e.g.,  }H_0:\rho_{xy} = .40)$
 
 - Transform statistic and null using Fisher's r to Z
 
 $$\large z^{'} = {\frac{1}{2}}ln{\frac{1+r}{1-r}}$$
 
 $$\large SE = \frac{1}{\sqrt{N-3}}$$
 
 ]
 
 
---

### Example

In PSY 302, the correlation between midterm exam grades and final exam grades was .56. The class size was 104. Is this statistically significant?

```{r, echo = F}
N = 104
r = .56

se = sqrt((1-r^2)/(N-2))

p1 = pt(r/se, df = N-2, lower.tail = F)
```


--
### Using t-method

$$\large SE_r = \sqrt{\frac{1-r^2}{N-2}} = \sqrt{\frac{1-.56^2}{104-2}} = `r round(se,2)`$$
$$\large t = \frac{r}{SE_r} = \frac{`r round(r,2)`}{`r round(se,2)`} = `r round(r/se,2)`$$

---


.left-column[
Probability of getting a *t* statistic of `r round(r/se,2)` or greater is `r p1`.
]

```{r, echo = F, message = F, warning = F, results='hide'}
library(tidyverse)
data.frame(x = c(-3,7)) %>%
  ggplot(aes(x = x)) +
  stat_function(fun = function(x) dt(x = x, df = N-2), geom = "line") +
  geom_vline(aes(xintercept = r/se), color = "purple") + 
  ggtitle("t distribution (DF = 102)")+
  theme_bw()
```

---

### Example

In PSY 302, the correlation between midterm exam grades and final exam grades was .56. The class size was 104. Is this statistically significantly different from .40?

--

```{r, echo = F}
r = .56
N = 104
null = .40
zr = psych::fisherz(r)
znull = psych::fisherz(null)
se = 1/sqrt(N-3)
stat = (zr-znull)/se
```

$$\large z^{'} = {\frac{1}{2}}ln{\frac{1+r}{1-r}}= {\frac{1}{2}}ln{\frac{1+`r r`}{1-`r r`}} = `r round(zr,2)`$$
$$\large z^{'}_{H_0} = {\frac{1}{2}}ln{\frac{1+r}{1-r}}= {\frac{1}{2}}ln{\frac{1+`r null`}{1-`r null`}} = `r round(znull,2)`$$
$$ SE_z = \frac{1}{\sqrt{`r N`-3}} = `r round(se,2)`$$
---

```{r, echo = c(1:4,6,8)}
r = .56
N = 104
null = .40
zr = psych::fisherz(r)
zr
znull = psych::fisherz(null)
znull
se = 1/sqrt(N-3)
se
```


---

$$Z_{\text{statistic}} = \frac{z'-\mu}{SE_z}=\frac{`r round(zr,2)`-`r round(znull,2)`}{`r round(se,2)`} = `r round(stat,2)`$$

```{r, echo = c(1, 3)}
stat = (zr-znull)/se
stat
pnorm(stat, lower.tail = F)*2
```


---

## Today

- visualizing correlations
- correlation matrices
- reliability


---
## Visualizing correlations

For a single correlation, best practice is to visualize the relationship using a scatterplot. A best fit line is advised, as it can help clarify the strength and direction of the relationship. 

[http://guessthecorrelation.com/](http://guessthecorrelation.com/)

---

```{r, echo = F}
library(datasauRus)
datasaurus_dozen %>%
  filter(dataset == "away") %>%
  ggplot(aes(x = x, y = y)) +
  geom_point()+
  ggtitle(expression(paste(M[X], "= 54.3 ", S[X], "= 16.8 ", 
                           M[Y], "= 47.8 ", S[Y], "= 26.9 ",
                           "R = -.06"))) +
  theme_bw(base_size = 15)
```


---

```{r, echo = F}
datasaurus_dozen %>%
  filter(dataset == "h_lines") %>%
  ggplot(aes(x = x, y = y)) +
  geom_point()+
  ggtitle(expression(paste(M[X], "= 54.3 ", S[X], "= 16.8 ", 
                           M[Y], "= 47.8 ", S[Y], "= 26.9 ",
                           "R = -.06"))) +
  theme_bw(base_size = 15)
```

---

```{r, echo = F}
datasaurus_dozen %>%
  filter(dataset == "x_shape") %>%
  ggplot(aes(x = x, y = y)) +
  geom_point()+
  ggtitle(expression(paste(M[X], "= 54.3 ", S[X], "= 16.8 ", 
                           M[Y], "= 47.8 ", S[Y], "= 26.9 ",
                           "R = -.06"))) +
  theme_bw(base_size = 15)
```

---

```{r, echo = F}
datasaurus_dozen %>%
  filter(dataset == "circle") %>%
  ggplot(aes(x = x, y = y)) +
  geom_point()+
  ggtitle(expression(paste(M[X], "= 54.3 ", S[X], "= 16.8 ", 
                           M[Y], "= 47.8 ", S[Y], "= 26.9 ",
                           "R = -.06"))) +
  theme_bw(base_size = 15)
```


---
```{r, echo = F}
datasaurus_dozen %>%
  filter(dataset == "wide_lines") %>%
  ggplot(aes(x = x, y = y)) +
  geom_point()+
  ggtitle(expression(paste(M[X], "= 54.3 ", S[X], "= 16.8 ", 
                           M[Y], "= 47.8 ", S[Y], "= 26.9 ",
                           "R = -.06"))) +
  theme_bw(base_size = 15)
```


---

```{r, echo = F}
datasaurus_dozen %>%
  filter(dataset == "bullseye") %>%
  ggplot(aes(x = x, y = y)) +
  geom_point()+
  ggtitle(expression(paste(M[X], "= 54.3 ", S[X], "= 16.8 ", 
                           M[Y], "= 47.8 ", S[Y], "= 26.9 ",
                           "R = -.06"))) +
  theme_bw(base_size = 15)
```

---
```{r, echo = F}
datasaurus_dozen %>%
  filter(dataset == "star") %>%
  ggplot(aes(x = x, y = y)) +
  geom_point(size = 2)+
  ggtitle(expression(paste(M[X], "= 54.3 ", S[X], "= 16.8 ", 
                           M[Y], "= 47.8 ", S[Y], "= 26.9 ",
                           "R = -.06"))) +
  theme_bw(base_size = 15)
```

---
```{r, echo = F}
datasaurus_dozen %>%
  filter(dataset == "dino") %>%
  ggplot(aes(x = x, y = y)) +
  geom_point(size = 2)+
  ggtitle(expression(paste(M[X], "= 54.3 ", S[X], "= 16.8 ", 
                           M[Y], "= 47.8 ", S[Y], "= 26.9 ",
                           "R = -.06"))) +
  theme_bw(base_size = 15)
```

---

## Correlation matrices

Correlations are both a descriptive and an inferential statistic. As a descriptive statistic, they're useful for understanding what's going on in a larger dataset. 

Like we use the `summary()` or `describe()` (psych) functions to examine our dataset _before we run any infernetial tests_, we should also look at the correlation matrix. 

---

```{r}
library(psych)
data(bfi)
head(bfi)
```

---

```{r}
cor(bfi)
```

---

```{r}
round(cor(bfi, use = "pairwise"),2)
```

---

```{r}
round(cor(bfi, use = "complete"),2)
```

---

With .purple[pairwise deletion], different sets of cases contribute to different correlations.  That maximizes the sample sizes, but can lead to problems if the data are missing for some systematic reason.

.purple[Listwise deletion] (often referred to in `R` as use complete cases) doesn't have the same issue of biasing correlations, but does result in smaller samples and potentially limited generalizability.

A good practice is comparing the different matrices; if the correlation values are very different, this suggests that the missingness that affects pairwise deletion is systematic.

---

```{r}
round(cor(bfi, use = "pairwise")- cor(bfi, use = "complete"),2)
```

---

## Types of missingness

Ideally our missingness is .purple[missing completely at random (MCAR)]. This means the probability of being missing is the same for all observations. If this is the case, our correlation estimates will be unbiased (if underpowered) and we're free to use them with no concerns (other than the usual).
* Aliens beam into a warehouse and randomly take some files.

However, our data might be .purple[missing at random (MAR)]. This means the probability of being missing is different between cases, and also the probability is related to variables we have observed. This is not great, but sometimes we can account for this using the variables we have observed (e.g., imputation, different estimation methods).
* Raccoons sneak into the warehouse and eat all the files by the door. 

---

## Types of missingness

It's a problem if our data is .purple[missing not at random (MNAR)]. The probability of being missing differs for reasons that are unknown to us. This is especially problematic if the reason is associated with the variables at the heart of our study. Sensitivity analyses might help us detect MNAR-ness and possibly define the limits of our study, but we can't adjust our data for this issue.
* Criminals break into the warehouse and steal files about themselves.

---
## Visualizing correlation matrices

A single correlation can be informative; a correlation matrix is more than the sum of its parts. 

Correlation matrices can be used to infer larger patterns of relationships. You may be one of the gifted who can look at a matrix of numbers and see those patterns immediately. Or you can use .purple[heat maps] to visualize correlation matrices. 

```{r, results = 'hide'}
library(corrplot)
```

---

```{r}
corrplot(cor(bfi, use = "pairwise"), method = "square")
```

---

![](images/comm plot-1.png)

.small[
[Beck, Condon, & Jackson, 2019](https://psyarxiv.com/857ev/)
]
---

## Factors that influence $r$ (and most other test statistics)

1. Restriction of range (GRE scores and success)

2. Very skewed distributions (smoking and health)

3. Non-linear associations

4. Measurement overlap (modality and content)

5. Reliability

---
## Reliability


Which would you rather have?

  - 1-item final exam versus 30-item?

  - assessment via trained clinician vs tarot cards?

  - fMRI during minor earthquake vs no earthquake?

--

All measurement includes error

- Score = true score + measurement error (CTT version)

- Reliability assesses the consistency of measurement; high reliability indicates less error

---

## Reliability

- Cannot correlate error (randomness) with something

- Because we do not measure our variables perfectly we get lower correlations compared to true correlations

- If we want to have a valid measure it better be a reliable measure

---
## Reliability

- think of reliability as a correlation with a measure and itself in a different world, at a different time, or a different but equal version

$$\large r_{XX}$$

---
## Reliability

- true score variance divided by observed variance
- how do you assess theoretical variance i.e., true score variance?

$$\large r_{XY} = r_{X_{T} Y_{T}} {\sqrt{r_{XX}r_{YY}}}$$

$$\large r_{XY} = .6 {\sqrt {(.70) (.70)}}$$

---
## Reliability

$$\large r_{X_{T} Y_{T}} =  = {\frac {r_{XY}} {\sqrt{r_{XX}r_{YY}}}}$$


$$\large r_{X_{T} Y_{T}} =  = {\frac {.30} {\sqrt{(.70)(.70)}}} = .42$$


???

### Take aways

N needed for .42 = 42
N needed for .3 = 84 -- need twice as many people!!

it doesn't work the other way -- you can't take your correlation and back calculate the true score, because reliabilities are also estimates. these can be wrong; the correlation you calculate is the max it could be

---
## Most common ways to assess

- Cronbach's alpha

```{r, eval=FALSE}
library(psych)
alpha(dataset[,items])
alpha(bfi[,c("A1", "A2", "A3", "A4", "A5")])
## Gives average split half correlation
## Can tell you if you are assessing a single construct
## Conflicts with tidyverse - fix with psych::alpha()
```

- Rest-retest reliability
- Kappa or ICC

---
## Reliability

- if you are going to measure something, do it well

- applies to ALL IVs and DVs, and all designs

- remember this when interpreting research

---
## Types of correlations

- Many ways to get at relationship between two variables

- Statistically the different types are _almost_ exactly the same
- Exist for historical reasons

---

## Types of correlations

1. Point Biserial
    +  continuous and dichotomous
2. Phi coefficient
    + both dichotomous
3. Spearman rank order
    + ranked data (nonparametric)
4. Biserial (assumes dichotomous is continuous)

Some important exceptions to the equivalence rule

5. Tetrachoric 
    + used for 2x2 contingency table
    + useful for assessing agreement between reviewers
6. Polychoric 
    + ordinal variables (Likert scales)
    + extension of tetrachoric
    
---

## Statistics and eugenics

The concept of the correlation is primarily attributed to Sir Frances Galton.
* He was also the founder of the [concept of eugenics](https://www.theguardian.com/commentisfree/2019/oct/03/eugenics-francis-galton-science-ideas).

The correlation coefficient was developed by his student, [Karl Pearson](https://www.britannica.com/biography/Karl-Pearson), and adapted into the ANOVA framework by [Sir Ronald Fisher](https://statmodeling.stat.columbia.edu/2020/08/01/ra-fisher-and-the-science-of-hatred/).
* Both were prominent advocates for the eugenics movement. 


---

## What do we do with this information?

* Never use the correlation or the later techniques developed on it? Of course not. 

* Acknowledge this history? Certainly.

* [Understand how the perspectives](https://medium.com/swlh/is-statistics-racist-59cd4ddb5fa9) of Galton, Fisher, Pearson and others [shaped our practices](http://gppreview.com/2019/12/16/eugenics-ethics-statistical-analysis/)? We must! -- these are not set in stone, [nor are they necessarily the best way](https://www.forbes.com/sites/jerrybowyer/2016/01/06/beer-vs-eugenics-the-good-and-the-bad-uses-of-statistics/?sh=3114a0c82a14) to move forward.
  * Statistical significance was a way to avoid talking about nuance or degree.
  * "Correlation does not imply causation" was a refutation of work demonstrating associations between environment and poverty.
  


---

class: inverse

## Next time....

Univariate regression

