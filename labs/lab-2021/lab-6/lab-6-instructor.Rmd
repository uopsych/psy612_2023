---
title: "Lab 6"
date: "2/9/2021"
output: html_document
editor_options: 
  chunk_output_type: console
---

You will need the Rmd file for today's lab - can download it [here](https://uopsych.github.io/psy612/labs/lab-6/lab-6.Rmd).

# Load Libraries

Install any packages you have not used before.

```{r}
library(rio)
library(tidyverse)
library(broom)
library(sjPlot)
library(olsrr)
library(gridExtra)
library(car)
```


# Import the Data

We'll be working with several versions of a dataset that includes participants' scores on happiness and extraversion. Each version has modified the data to represent one of the violations of the assumptions underlying linear regression.

```{r}
df <- import("https://raw.githubusercontent.com/uopsych/psy612/master/labs/lab-2/data/PSY612_Lab2_Data.csv") %>% janitor::clean_names() %>% select(extraversion, happiness) # to get things into snake_case

df2 <- import("https://raw.githubusercontent.com/uopsych/psy612/master/labs/lab-6/data/df_nl.csv")

df3 <- import("https://raw.githubusercontent.com/uopsych/psy612/master/labs/lab-6/data/df_nne.csv")

df4 <- import("https://raw.githubusercontent.com/uopsych/psy612/master/labs/lab-6/data/df_het.csv")

df5 <- import("https://raw.githubusercontent.com/uopsych/psy612/master/labs/lab-6/data/lab6_outliers.txt") %>% 
  janitor::clean_names()

df6 <- import("https://raw.githubusercontent.com/uopsych/psy612/master/labs/lab-6/data/df_mc.csv")
```


# Inspect the Data

The same variables (mostly) are contained in each dataset. Let's inspect just the df dataset.
```{r}
head(df)
str(df)
```


# Visualize Each Dataset

We're going to focus on the relationship between happiness (Y) and extraversion (X). Let's visualize this using a scatterplot for each dataset. Visualization is always a good place to start when inspecting the relationships between variables.
```{r}
# Scatterplot using df
scatter1 <- ggplot(data = df, aes(x = extraversion, y = happiness)) +
  geom_point() + 
  geom_smooth(method = "lm") +
  labs(x = "Extraversion", y = "Happiness", title = "df") +
  theme(plot.title = element_text(hjust = 0.5)) # Center the title

# Scatterplot using df2
scatter2 <- ggplot(data = df2, aes(x = extraversion, y = happiness)) +
  geom_point() + 
  geom_smooth(method = "lm") +
  labs(x = "Extraversion", y = "Happiness", title = "df2") +
  theme(plot.title = element_text(hjust = 0.5))

# Scatterplot using df3
scatter3 <- ggplot(data = df3, aes(x = extraversion, y = happiness)) +
  geom_point() + 
  geom_smooth(method = "lm") +
  labs(x = "Extraversion", y = "Happiness", title = "df3") +
  theme(plot.title = element_text(hjust = 0.5))

# Scatterplot using df4
scatter4 <- ggplot(data = df4, aes(x = extraversion, y = happiness)) +
  geom_point() + 
  geom_smooth(method = "lm") +
  labs(x = "Extraversion", y = "Happiness", title = "df4") +
  theme(plot.title = element_text(hjust = 0.5))

# Scatterplot using df5
scatter5 <- ggplot(data = df5, aes(x = extraversion, y = happiness)) +
  geom_point() + 
  geom_smooth(method = "lm") +
  labs(x = "Extraversion", y = "Happiness", title = "df5") +
  theme(plot.title = element_text(hjust = 0.5))

# Scatterplot using df6
scatter6 <- ggplot(data = df6, aes(x = extraversion, y = happiness)) +
  geom_point() + 
  geom_smooth(method = "lm") +
  labs(x = "Extraversion", y = "Happiness", title = "df6") +
  theme(plot.title = element_text(hjust = 0.5))

```


Let's look at them side-by-side.
```{r}
grid.arrange(scatter1, scatter2, scatter3, scatter4, scatter5, scatter6)
```

### Q1
>**Question: Do you notice any potential issues from any of these scatterplots that we used to represent the data so far?**



# Potential Issues in the Data

Some of the potential issues that we can run into in the data that would violate one or more of the assumptions underlying the linear regression analysis include (along with ways of investigating each):

1. Non-linearity 
- Examine a plot of the fitted values vs residuals

2. Non-normally distributed errors 
- a) Examine the distribution of the residuals, b) examine a QQ-plot

3. Heteroscedasticity 
- Examine a plot of the fitted values vs residuals

4. Non-independence among errors
- Examine the relationship between the residuals and a variable the errors should not correspond to (e.g., ID number)

5. Outliers 
- Univariate outliers: Examine a histogram or boxplot
- Multivariate outliers: Examine a scatterplot, cook's D, dfbetas, and dffits

6. Multicollinearity 
- Examine zero-order correlations, tolerances, and VIFs



# Running the linear regression models
```{r}
model1 <- lm(happiness ~ extraversion, data = df)
model2 <- lm(happiness ~ extraversion, data = df2)
model3 <- lm(happiness ~ extraversion, data = df3)
model4 <- lm(happiness ~ extraversion, data = df4)
model5 <- lm(happiness ~ extraversion, data = df5)
```


# Checking for non-linearity

To examine whether there is the presence of a non-linear trend in the relationship between X and Y that is not captured by our linear model, you can first examine a scatterplot (as we did above). 

Additionally, you can examine a plot of the model's fitted values by the residuals. We can obtain this plot using the function plot(), which is included in base R. Give the model as an input.

The plot() function provides a set of four different diagnostic plots. Let's look at them all first for model1. 
```{r}
par(mfrow = c(2, 2)) # display plots in a 2x2 grid
plot(model1)
```

Let's focus just on examining the plot of the fitted values by the residuals, which is the first plot.


Checking for non-linearity in model 1
```{r}
par(mfrow = c(2, 1))
plot(model1, 1)
```


Now, check for non-linearity in model2.
```{r}
plot(model2, 1)
```

### Q2
>**Question: Do you see a pattern in either of these plots that suggest the presence of a non-linear trend between income and happiness that is not captured by the linear model?**





# Checking for normally distributed errors

You can check whether the model's residuals are normally distributed by plotting either: a) a distribution of the residuals, b) a Q-Q plot


Remember we can pass our model to the augment() function to retrieve the model residuals. Let's run the augment() function on each mdoel here so we can use the output later. 
```{r}
aug_model1 <- augment(model1)
str(aug_model1)

aug_model2 <- augment(model2)
aug_model3 <- augment(model3)
aug_model4 <- augment(model4)
aug_model5 <- augment(model5)
```



Let's check whether the residuals are normally distributed for model 1. First, we'll use a density plot with a normal distribution overlaid on top to easily compare the two.

Density Plot
```{r}
ggplot(data = aug_model1, aes(x = .resid)) + # Plot the residuals
  geom_density(fill = "purple") + # Density plot
  stat_function(linetype = 2, fun = dnorm, # Add a normal curve (fun = dnorm) as a dashed line (linetype = 2)
                args = list(mean = mean(aug_model1$.resid), # define mean and sd to line up the normal curve with the plot of the residuals
                            sd = sd(aug_model1$.resid))) +
  theme_minimal()
```

Next, let's use a Q-Q plot.

QQ-Plot
```{r}
graphics.off() # Returns the plotting dimensions back to the default

ggplot(model1) +
  stat_qq(aes(sample = .stdresid)) + # Plot a qq-plot
  geom_abline() + # Add a diagonal line
  theme_minimal()
```

A qq-plot plots the quantiles of a reference distribution (in this case, the normal distribution) on the x-axis by the quantiles of the residuals on the y-axis. If your residuals come from a normal distribution, the percentage of data lying between any two points should approximate the percentage of data lying between any two points on a normal distribution.

When the points on a QQ-plot are close to lying on the diagonal line (with a slope of 1) the residuals are approximately normally distributed. 



Now, do the same thing, but this time let's examine whether we have violated the assumption of normally distributed residuals for model 3.

Obtain a density plot of the residuals from model 3:
```{r}
ggplot(data = aug_model3, aes(x = .resid)) + 
  geom_density(fill = "purple") +
  stat_function(linetype = 2, fun = dnorm, # add normal curve
                args = list(mean = mean(aug_model3$.resid), # define mean and sd
                            sd = sd(aug_model3$.resid)))+
  theme_minimal()
```


Obtain a Q-Q plot for model 3:
```{r}
ggplot(model3) +
  stat_qq(aes(sample = .stdresid)) +
  geom_abline() +
  theme_minimal()
```


### Q3
>**Question: Do the errors appear to be approximately normally distributed?**





# Checking for heteroscedasticity

The plot of the fitted values by the residuals for a model can also be used to check whether the assumption of homogeneity of variance has been met, aka whether error variance appears to be approximately the same across all values fitted by the model. 

Let's check for heteroscedasticity by examining the plot of the fitted values vs. residuals for model1. 
```{r}
plot(model1, 1)
```

Now, heck for heteroscedasticity by examining the plot of the fitted values vs. residuals for model4. 
```{r}
plot(model4, 1)
```

### Q4
>**Question: Does the amount of variability in the residuals seem to differ across the model's fitted values for either of these models (aka, is there evidence of heteroscedasticity)?**





# Checking for non-independence among errors

One way to check for non-independence among errors is to examine whether the errors have a non-random (aka, systematic) relationship with a variable they shouldn't, such as ID number (which indicates the order in which participants participated in the study), season, month, etc.

Let's check whether we have violated the assumption of independence of errors by plotting residuals against ID numbers for model 1.
```{r}
aug_model1$ID <- c(1:nrow(aug_model1)) # Add a column of ID numbers to the object containing the model residuals

ggplot(data = aug_model1, 
       aes(x=ID, y = .resid)) + 
  geom_point() +  
  geom_smooth(se = F) +
  geom_hline(yintercept = 0)
```


Repeat the same thing, but this time testing the independence of errors assumption for model 2.
```{r}
aug_model4$ID <- c(1:nrow(aug_model4))

ggplot(data = aug_model4, 
       aes(x=ID, y = .resid)) + 
  geom_point() +  
  geom_smooth(se = F) +
  geom_hline(yintercept = 0)
```

### Q5
>**Question: Does there appear to be a relationship between ID number and the residuals for either model above?**





# Checking for outliers

The presence of outliers means a few values could be having a disproportionate influence on a single variable (univariate outliers) or on the fit of a regression model (multivariate outliers). Therefore, we want to identify outliers so we can consider whether they should be removed or changed. 


## Univariate outliers

One way of visually seeing potential outliers for a single variable is by using a boxplot.

Let's obtain a boxplot of the happiness outcome variable from the dataset called df5.
```{r}
ggplot(df5) +
  aes(y = happiness) +
  geom_boxplot() +
  theme_minimal()
```

The outer edges of the boxplot correspond to the first and third quartiles (the values partitioning where the middle 50% of the data is located).

The upper whisker extends from the top edge of the boxplot to the largest value no further than 1.5 * IQR from the edge (recall that IQR is the distance from the first to the third quartile). The lower whisker extends from the lower edge of the boxplot to the smallest value that is smallest value that is no further than 1.5 * IQR from the edge Data points beyond these are considered outliers.

To determine the value of the potential outliers, you can use boxplot.stats(). The value(s) underneath $out are scores on the variable that landed more extreme than the boxplot's whiskers.
```{r}
boxplot.stats(df5$happiness)
```

Let's find out who in the original dataset this value corresponds to.
```{r}
df5$ID <- c(1:nrow(df5)) # Add a column of ID numbers

df5[df5$happiness == 32,] # Use indexing [row, column] to grab the row for the person whose score on happiness is equal to the value of the outlier
```

Not all outliers determined in this way necessarily need to be removed from or changed in the dataset. Use your knowledge of your field, the methods used, and how extreme the outlier is to determine if it should be addressed. You can also examine the mean of your variable with and without the outlier present to gauge how strong of an influence it is having. 




## Multivariate outliers

You can also have multivariate outliers, which are outliers because they are far away from the values predicted by your linear regression model *or* they have an undue influence on the fit of the model. 

One straightforward way to visually inspect the data for multivariate outliers is to use bi-variate scatterplots for each combination of outcome and predictor. 

Let's look again at the scatterplot of the relationship between extraversion in happiness from df5.
```{r}
scatter5 <- ggplot(data = df5, aes(x = extraversion, y = happiness)) +
  geom_point() + 
  geom_smooth(method = "lm") +
  labs(x = "Extraversion", y = "Happiness", title = "df5") +
  theme(plot.title = element_text(hjust = 0.5))

scatter5
```

### Q6
>**Question: Do there appear to be any multivariate outliers in the scatterplot?**





We can get a quantitative measure of the *distance* of points from the regression model using either 1) Standardized residuals or 2) Studentized residuals.

# Standardized Residuals 

Standardized residuals are the raw residuals put into standardized units by dividing by the residual standard error.

Plot
```{r}
ols_plot_resid_stand(model5)
```

Values
```{r}
aug_model5$ID <- c(1:nrow(aug_model5))

aug_model5 %>% 
  arrange(desc(abs(.std.resid)))%>% # Arrange the dataset by highest to lowest standardized residuals
  head(n = 20)
```

One standard I have seen is that standardized residuals greater than +/- 2 are worth inspecting further.



# Studentized Residuals

Studentized residuals are calculatd by fitting a regression model, deleting each point one at a time, and calculating the distance between that data point and the fitted value from the model fit without it, standardized. 

Plot
```{r}
ols_plot_resid_stud(model5)
```

Values
```{r}
studentized_resids <- rstudent(model5)
sort(desc(abs(studentized_resids)))
```

One standard I have seen is that studentized residuals greater than +/- 3 are worth inspecting further.



Another way of examining whether outliers are present and should be dealt with is by measuring their *influence* on the regression model. Three measures you can use to measure influence of a data point on the regression model are 1) Cook's Distance, 2) DFFITS, and 3) DFBETAS


# Cook's Distance

Summarizes how much the regression model would change if you removed a particular value (measures the difference between the regression coefficients obtained from the full dataset to the regression coefficients that would be obtained if you deleted that particular value).

Plot
```{r}
ols_plot_cooksd_chart(model5)
```

Values
```{r}
aug_model5 %>% 
  arrange(desc(abs(.cooksd)))%>% # Arrange the dataset by highest to lowest Cook's D
  head(n = 20)
```

One standard I have seen is that Cook's D values greater than 3 times the average Cook's D values are worth investigating.



#DFBETAS

DFBETA measures the difference in each parameter estimate with and without a particular point being included in the data. 

Plot
```{r}
ols_plot_dfbetas(model5)
```

Value
```{r}
dfbetas <- dfbeta(model5)
sort(desc(abs(dfbetas)))
```

One standard I have seen is that DFBETAS larger than 2/sqrt(n) should be investigated as potential outliers.



# DFFITS

DFFITS measure the difference in the model's fitted values with and without a particular point being included in the data. 

Plot
```{r}
ols_plot_dffits(model5)
```

Values
```{r}
dffits <- dffits(model5)
sort(desc(abs(dffits)))
```

One standard I have seen is that DFFITS larger than 2∗(p+1)/√(n−p−1) should be investigated as potential outliers (n = number of observations, p = number of predictors including the intercept).




# Using sjPlot to easily obtain Regression Diagnostic Plots

Lastly, there is another handy function called plot_model() in the sjPlot package that is another option for easily obtaining a variety of regression diagnostic plots. 

Within the plot_model() function, provide the argument type = "diag" in order to obtain the regression diagnostics plots. 
```{r}
plot_model(model5, type = "diag")
```



# Checking for multicollinearity

Multicollinearity occurs when one or more predictors in our model are highly correlated. 


One way we can check this before we even run our model is to look at the zero-order correlations between variables. The df6 dataset contains two predictor variables. Let's investigate the correlation between them.

```{r}
head(df6)
cor(df6$extraversion, df6$social_engagement)
```

Extraversion and social engagement appear to be VERY highly correlated (r = 0.96)!



#VIF (Variance Inflation Factor) / Tolerance

Recall two measures of multicollinearity that we have, tolerance and VIF, where

Tolerance is: $$1 - R_{12}^2$$

And VIF (variance inflation factor) is $$1/Tolerance$$

Let's run a model with two predictors using df6. We'll predict happiness from both extraversion and social engagement, two predictors that sound like they have the potential to be correlated.
```{r}
model6 <- lm(happiness ~ extraversion + social_engagement, data = df6)
```


When there are multiple predictors in a model, the output of the plot_model() function will also provide a plot of VIFs.
```{r}
plot_model(model6, type = "diag")
```

Values 
```{r}
ols_vif_tol(model6)
```

Either a *low* tolerance (below 0.20 is one standard I have seen) or a *high* VIF (above 5 or 10 are standards I have seen) is an indication of a problem with multicollinearity.



# Minihack 1

Load the bfi dataset from the psych package (this is done for you below, in case you haven't loaded a dataset from a package).

First, create a model regressing age on the 5 items for Conscientiousness (labeled C1, C2, C3, C4, and C5).

```{r}
data(bfi, package = "psych")

model <- lm(age ~ C1 + C2 + C3 + C4 + C5, data = bfi) 
```


Next, check if the data meet the homoscedasticity assumption.

```{r}
model %>% 
  augment() %>% 
  ggplot(aes(x = .fitted, y = .resid)) +
  geom_point() +
  theme_minimal() +
  geom_hline(yintercept = 0, color = "red")
```

Next, check if the errors are normally distributed using a plot.

```{r}
# multiple options here. I'll use plot_model and just grab the 3rd plot,
# which has the distribution of the resdiuals
plot_model(model, type = "diag")[[3]]
```


# Minihack 2

Using the `df5` data from above, run a regression model with and without the outliers we identified. 
```{r}
model_w_out <- lm(happiness ~ extraversion, data = df_out)
model_no_out <- df_out[-c(134, 149, 161, 165), ] %>% 
  lm(happiness ~ extraversion, data = .)
```

Compare the output of the two models. How similar/different are they? What values change?



# Minihack 3

Using the fitness data from the olsrr package (loaded for you below), regress `runtime` on `weight`, `restpulse`, `runpulse`, and `maxpulse`.

```{r}
data("fitness") # load data
model_3 <- lm(runtime ~ weight + restpulse + runpulse + maxpulse, data = fitness)
```

Check the multicollinearity of predictors. Is multicollinearity at a tolerable level? 

```{r}
model_3 %>% 
  ols_vif_tol()
```

